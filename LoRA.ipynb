{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8108,
     "status": "ok",
     "timestamp": 1582965084445,
     "user": {
      "displayName": "Rohin Garg",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAn2DI9Fvm4QY6q8HS8NkUchCJTPc3yLcioW7GVvA=s64",
      "userId": "03455532354584450462"
     },
     "user_tz": -330
    },
    "id": "sKb4S7vqyepJ",
    "outputId": "2a0e401f-4f78-4523-aad5-aec2b0abf99e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['NLTK_DATA'] = 'nltk_data'\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "from transformers import *\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "# import tensorflow as tf\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import sentencepiece\n",
    "# from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 实际使用的GPU设备 =====\n",
      "可见GPU数量: 1\n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "There is/are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "# 验证实际可见的GPU设备\n",
    "print(\"\\n===== 实际使用的GPU设备 =====\")\n",
    "print(f\"可见GPU数量: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    \n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    # device = torch.device(\"cuda\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print('There is/are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible. Somehow this isn't working!\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# 创建保存模型和预测的文件夹\n",
    "os.makedirs('./saved_preds', exist_ok=True)\n",
    "os.makedirs('./saved_models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义集成模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义本地模型路径\n",
    "LOCAL_MODEL_PATHS = {\n",
    "    'bert-large-uncased': 'bert-large-uncased',\n",
    "    'roberta-large': 'roberta-large',\n",
    "    'xlnet-large-cased': 'xlnet-large-cased'\n",
    "}\n",
    "\n",
    "# 启用多个模型配置\n",
    "MODELS = [\n",
    "    (BertForSequenceClassification, BertTokenizer, 'bert-large-uncased'),\n",
    "    (RobertaForSequenceClassification, RobertaTokenizer, 'roberta-large'),\n",
    "    (XLNetForSequenceClassification, XLNetTokenizer, 'xlnet-large-cased'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = pd.read_csv(\"subtask1_test.csv\", encoding='utf-8')\n",
    "master_corpus = pd.read_csv(\"subtask1_train_with_tense.csv\", encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file tokenizer.json\n",
      "loading configuration file bert-large-uncased/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-large-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file bert-large-uncased/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file bert-large-uncased/model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== Training Model with LoRA: bert-large-uncased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,557,378 || all params: 338,701,316 || trainable%: 1.0503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4874/3881127844.py:21: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  self.tense_weights = torch.tensor([total / tense_counts[i] for i in range(len(tense_counts))], dtype=torch.float32).to(device)\n",
      "/tmp/ipykernel_4874/3881127844.py:30: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  self.tense_weights = torch.tensor([total / tense_counts[i] for i in range(len(tense_counts))], dtype=torch.float32).to(device)\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.4379\n",
      "Main Task - Accuracy: 0.8954, Precision: 0.7199, Recall: 0.8008, F1: 0.7582\n",
      "Tense Task - Accuracy: 0.3188\n",
      "New best F1 score: 0.8573\n",
      "Epoch 2 - Loss: 0.1829\n",
      "Main Task - Accuracy: 0.9655, Precision: 0.8964, Recall: 0.9401, F1: 0.9177\n",
      "Tense Task - Accuracy: 0.5028\n",
      "New best F1 score: 0.8650\n",
      "Epoch 3 - Loss: 0.1322\n",
      "Main Task - Accuracy: 0.9810, Precision: 0.9402, Recall: 0.9687, F1: 0.9543\n",
      "Tense Task - Accuracy: 0.5619\n",
      "Epoch 4 - Loss: 0.0940\n",
      "Main Task - Accuracy: 0.9899, Precision: 0.9685, Recall: 0.9828, F1: 0.9756\n",
      "Tense Task - Accuracy: 0.6123\n",
      "Epoch 5 - Loss: 0.0667\n",
      "Main Task - Accuracy: 0.9956, Precision: 0.9869, Recall: 0.9916, F1: 0.9893\n",
      "Tense Task - Accuracy: 0.6526\n",
      "Epoch 6 - Loss: 0.0488\n",
      "Main Task - Accuracy: 0.9975, Precision: 0.9916, Recall: 0.9960, F1: 0.9938\n",
      "Tense Task - Accuracy: 0.6896\n",
      "Epoch 7 - Loss: 0.0363\n",
      "Main Task - Accuracy: 0.9988, Precision: 0.9953, Recall: 0.9987, F1: 0.9970\n",
      "Tense Task - Accuracy: 0.7226\n",
      "New best F1 score: 0.8705\n",
      "Epoch 8 - Loss: 0.0300\n",
      "Main Task - Accuracy: 0.9991, Precision: 0.9976, Recall: 0.9980, F1: 0.9978\n",
      "Tense Task - Accuracy: 0.7439\n",
      "Epoch 9 - Loss: 0.0228\n",
      "Main Task - Accuracy: 0.9993, Precision: 0.9976, Recall: 0.9990, F1: 0.9983\n",
      "Tense Task - Accuracy: 0.7614\n",
      "Epoch 10 - Loss: 0.0184\n",
      "Main Task - Accuracy: 0.9995, Precision: 0.9983, Recall: 0.9993, F1: 0.9988\n",
      "Tense Task - Accuracy: 0.7765\n",
      "New best F1 score: 0.8710\n",
      "\n",
      "Final Test Results with LoRA:\n",
      "Main Task - Accuracy: 0.9727, Precision: 0.8681, Recall: 0.8740, F1: 0.8710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file bert-large-uncased/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file tokenizer.json\n",
      "loading configuration file roberta-large/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file roberta-large/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file roberta-large/model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LoRA model and predictions for bert-large-uncased\n",
      "Best validation F1 score: 0.8710\n",
      "\n",
      "\n",
      "=== Training Model with LoRA: roberta-large ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,606,978 || all params: 359,985,156 || trainable%: 1.2798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4874/3881127844.py:21: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  self.tense_weights = torch.tensor([total / tense_counts[i] for i in range(len(tense_counts))], dtype=torch.float32).to(device)\n",
      "/tmp/ipykernel_4874/3881127844.py:30: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  self.tense_weights = torch.tensor([total / tense_counts[i] for i in range(len(tense_counts))], dtype=torch.float32).to(device)\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.3739\n",
      "Main Task - Accuracy: 0.8956, Precision: 0.7206, Recall: 0.8008, F1: 0.7586\n",
      "Tense Task - Accuracy: 0.3439\n",
      "New best F1 score: 0.8586\n",
      "Epoch 2 - Loss: 0.1738\n",
      "Main Task - Accuracy: 0.9682, Precision: 0.9057, Recall: 0.9431, F1: 0.9240\n",
      "Tense Task - Accuracy: 0.5293\n",
      "New best F1 score: 0.8643\n",
      "Epoch 3 - Loss: 0.1238\n",
      "Main Task - Accuracy: 0.9816, Precision: 0.9416, Recall: 0.9704, F1: 0.9558\n",
      "Tense Task - Accuracy: 0.5775\n",
      "New best F1 score: 0.8885\n",
      "Epoch 4 - Loss: 0.0933\n",
      "Main Task - Accuracy: 0.9884, Precision: 0.9627, Recall: 0.9815, F1: 0.9720\n",
      "Tense Task - Accuracy: 0.6153\n",
      "Epoch 5 - Loss: 0.0722\n",
      "Main Task - Accuracy: 0.9947, Precision: 0.9820, Recall: 0.9923, F1: 0.9871\n",
      "Tense Task - Accuracy: 0.6409\n",
      "New best F1 score: 0.8959\n",
      "Epoch 6 - Loss: 0.0559\n",
      "Main Task - Accuracy: 0.9962, Precision: 0.9867, Recall: 0.9950, F1: 0.9908\n",
      "Tense Task - Accuracy: 0.6778\n",
      "Epoch 7 - Loss: 0.0433\n",
      "Main Task - Accuracy: 0.9980, Precision: 0.9930, Recall: 0.9973, F1: 0.9951\n",
      "Tense Task - Accuracy: 0.7051\n",
      "Epoch 8 - Loss: 0.0315\n",
      "Main Task - Accuracy: 0.9993, Precision: 0.9973, Recall: 0.9993, F1: 0.9983\n",
      "Tense Task - Accuracy: 0.7325\n",
      "New best F1 score: 0.8995\n",
      "Epoch 9 - Loss: 0.0252\n",
      "Main Task - Accuracy: 0.9994, Precision: 0.9980, Recall: 0.9990, F1: 0.9985\n",
      "Tense Task - Accuracy: 0.7542\n",
      "Epoch 10 - Loss: 0.0201\n",
      "Main Task - Accuracy: 0.9997, Precision: 0.9987, Recall: 0.9997, F1: 0.9992\n",
      "Tense Task - Accuracy: 0.7689\n",
      "\n",
      "Final Test Results with LoRA:\n",
      "Main Task - Accuracy: 0.9790, Precision: 0.9133, Recall: 0.8848, F1: 0.8988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file roberta-large/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file spiece.model\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file tokenizer.json\n",
      "loading configuration file xlnet-large-cased/config.json\n",
      "Model config XLNetConfig {\n",
      "  \"_name_or_path\": \"xlnet-large-cased\",\n",
      "  \"architectures\": [\n",
      "    \"XLNetLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_type\": \"bi\",\n",
      "  \"bi_data\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"clamp_len\": -1,\n",
      "  \"d_head\": 64,\n",
      "  \"d_inner\": 4096,\n",
      "  \"d_model\": 1024,\n",
      "  \"dropout\": 0.1,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ff_activation\": \"gelu\",\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mem_len\": null,\n",
      "  \"model_type\": \"xlnet\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_layer\": 24,\n",
      "  \"pad_token_id\": 5,\n",
      "  \"reuse_len\": null,\n",
      "  \"same_length\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": \"tanh\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"last\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 250\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.41.1\",\n",
      "  \"untie_r\": true,\n",
      "  \"use_mems_eval\": true,\n",
      "  \"use_mems_train\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file xlnet-large-cased/config.json\n",
      "Model config XLNetConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLNetLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_type\": \"bi\",\n",
      "  \"bi_data\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"clamp_len\": -1,\n",
      "  \"d_head\": 64,\n",
      "  \"d_inner\": 4096,\n",
      "  \"d_model\": 1024,\n",
      "  \"dropout\": 0.1,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ff_activation\": \"gelu\",\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mem_len\": null,\n",
      "  \"model_type\": \"xlnet\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_layer\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 5,\n",
      "  \"reuse_len\": null,\n",
      "  \"same_length\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": \"tanh\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"last\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 250\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.41.1\",\n",
      "  \"untie_r\": true,\n",
      "  \"use_mems_eval\": true,\n",
      "  \"use_mems_train\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file xlnet-large-cased/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LoRA model and predictions for roberta-large\n",
      "Best validation F1 score: 0.8995\n",
      "\n",
      "\n",
      "=== Training Model with LoRA: xlnet-large-cased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_4874/3881127844.py:21: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  self.tense_weights = torch.tensor([total / tense_counts[i] for i in range(len(tense_counts))], dtype=torch.float32).to(device)\n",
      "/tmp/ipykernel_4874/3881127844.py:30: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  self.tense_weights = torch.tensor([total / tense_counts[i] for i in range(len(tense_counts))], dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding XLNet modules...\n",
      "Found linear modules: ['logits_proj', 'layer_2', 'layer_1', 'summary']\n",
      "trainable params: 1,990,672 || all params: 363,311,122 || trainable%: 0.5479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.6347\n",
      "Main Task - Accuracy: 0.8268, Precision: 0.5584, Recall: 0.7355, F1: 0.6348\n",
      "Tense Task - Accuracy: 0.2951\n",
      "New best F1 score: 0.8282\n",
      "Epoch 2 - Loss: 0.1993\n",
      "Main Task - Accuracy: 0.9636, Precision: 0.8952, Recall: 0.9314, F1: 0.9129\n",
      "Tense Task - Accuracy: 0.4864\n",
      "New best F1 score: 0.8810\n",
      "Epoch 3 - Loss: 0.1458\n",
      "Main Task - Accuracy: 0.9771, Precision: 0.9342, Recall: 0.9552, F1: 0.9446\n",
      "Tense Task - Accuracy: 0.5541\n",
      "New best F1 score: 0.8850\n",
      "Epoch 4 - Loss: 0.1094\n",
      "Main Task - Accuracy: 0.9864, Precision: 0.9608, Recall: 0.9731, F1: 0.9669\n",
      "Tense Task - Accuracy: 0.5955\n",
      "New best F1 score: 0.8926\n",
      "Epoch 5 - Loss: 0.0826\n",
      "Main Task - Accuracy: 0.9913, Precision: 0.9731, Recall: 0.9849, F1: 0.9789\n",
      "Tense Task - Accuracy: 0.6245\n",
      "Epoch 6 - Loss: 0.0646\n",
      "Main Task - Accuracy: 0.9951, Precision: 0.9827, Recall: 0.9936, F1: 0.9881\n",
      "Tense Task - Accuracy: 0.6593\n",
      "Epoch 7 - Loss: 0.0526\n",
      "Main Task - Accuracy: 0.9972, Precision: 0.9913, Recall: 0.9953, F1: 0.9933\n",
      "Tense Task - Accuracy: 0.6821\n",
      "Epoch 8 - Loss: 0.0403\n",
      "Main Task - Accuracy: 0.9986, Precision: 0.9943, Recall: 0.9990, F1: 0.9966\n",
      "Tense Task - Accuracy: 0.7070\n",
      "New best F1 score: 0.8929\n",
      "Epoch 9 - Loss: 0.0348\n",
      "Main Task - Accuracy: 0.9987, Precision: 0.9953, Recall: 0.9983, F1: 0.9968\n",
      "Tense Task - Accuracy: 0.7279\n",
      "New best F1 score: 0.8978\n",
      "Epoch 10 - Loss: 0.0312\n",
      "Main Task - Accuracy: 0.9992, Precision: 0.9980, Recall: 0.9983, F1: 0.9981\n",
      "Tense Task - Accuracy: 0.7385\n",
      "\n",
      "Final Test Results with LoRA:\n",
      "Main Task - Accuracy: 0.9777, Precision: 0.8890, Recall: 0.9011, F1: 0.8950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file xlnet-large-cased/config.json\n",
      "Model config XLNetConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLNetLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_type\": \"bi\",\n",
      "  \"bi_data\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"clamp_len\": -1,\n",
      "  \"d_head\": 64,\n",
      "  \"d_inner\": 4096,\n",
      "  \"d_model\": 1024,\n",
      "  \"dropout\": 0.1,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ff_activation\": \"gelu\",\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mem_len\": null,\n",
      "  \"model_type\": \"xlnet\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_layer\": 24,\n",
      "  \"pad_token_id\": 5,\n",
      "  \"reuse_len\": null,\n",
      "  \"same_length\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": \"tanh\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"last\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 250\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.41.1\",\n",
      "  \"untie_r\": true,\n",
      "  \"use_mems_eval\": true,\n",
      "  \"use_mems_train\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LoRA model and predictions for xlnet-large-cased\n",
      "Best validation F1 score: 0.8978\n"
     ]
    }
   ],
   "source": [
    "# Multi-Task Learning with LoRA\n",
    "\n",
    "# 数据集类保持不变\n",
    "class MultiTaskClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, corpus, train_corpus, tokenizer, max_len=128, is_test=False):\n",
    "        self.corpus = corpus\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # 计算主任务的类别权重\n",
    "        label_counts = train_corpus['gold_label'].value_counts()\n",
    "        total = len(train_corpus)\n",
    "        self.main_task_weights = torch.tensor([total / label_counts[i] for i in range(len(label_counts))], dtype=torch.float32).to(device)\n",
    "        \n",
    "        # 只在训练集上计算辅助任务的权重\n",
    "        if not is_test and 'tense' in corpus.columns:\n",
    "            # 计算辅助任务（时态）的类别权重\n",
    "            tense_counts = train_corpus['tense'].value_counts()\n",
    "            self.num_tenses = len(tense_counts)\n",
    "            self.tense_weights = torch.tensor([total / tense_counts[i] for i in range(len(tense_counts))], dtype=torch.float32).to(device)\n",
    "            \n",
    "            # 创建时态到索引的映射\n",
    "            self.tense_to_idx = {tense: idx for idx, tense in enumerate(sorted(train_corpus['tense'].unique()))}\n",
    "        else:\n",
    "            # 对于测试集，我们需要从训练集获取时态信息\n",
    "            if 'tense' in train_corpus.columns:\n",
    "                tense_counts = train_corpus['tense'].value_counts()\n",
    "                self.num_tenses = len(tense_counts)\n",
    "                self.tense_weights = torch.tensor([total / tense_counts[i] for i in range(len(tense_counts))], dtype=torch.float32).to(device)\n",
    "                self.tense_to_idx = {tense: idx for idx, tense in enumerate(sorted(train_corpus['tense'].unique()))}\n",
    "            else:\n",
    "                self.num_tenses = 0\n",
    "                self.tense_weights = None\n",
    "                self.tense_to_idx = {}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.corpus)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.corpus.iloc[idx]\n",
    "        text = row['sentence']\n",
    "        main_label = row['gold_label']\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'main_label': torch.tensor(main_label, dtype=torch.long),\n",
    "        }\n",
    "        \n",
    "        # 只在训练时添加时态标签\n",
    "        if not self.is_test and 'tense' in row.index:\n",
    "            tense_label = self.tense_to_idx[row['tense']]\n",
    "            result['tense_label'] = torch.tensor(tense_label, dtype=torch.long)\n",
    "        else:\n",
    "            # 对于测试集，使用一个占位符\n",
    "            result['tense_label'] = torch.tensor(0, dtype=torch.long)\n",
    "            \n",
    "        return result\n",
    "\n",
    "# 创建一个支持LoRA的多任务模型包装器\n",
    "class MultiTaskLoRAModel(nn.Module):\n",
    "    def __init__(self, base_model, num_main_labels=2, num_tense_labels=None):\n",
    "        super(MultiTaskLoRAModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.num_main_labels = num_main_labels\n",
    "        self.num_tense_labels = num_tense_labels\n",
    "        \n",
    "        # 获取隐藏层大小\n",
    "        hidden_size = base_model.config.hidden_size\n",
    "        \n",
    "        # 主任务分类头\n",
    "        self.main_classifier = nn.Linear(hidden_size, num_main_labels)\n",
    "        \n",
    "        # 辅助任务（时态）分类头\n",
    "        self.tense_classifier = nn.Linear(hidden_size, num_tense_labels)\n",
    "        \n",
    "        # Dropout层\n",
    "        dropout = getattr(base_model.config, 'hidden_dropout_prob', None)\n",
    "        if dropout is None:\n",
    "            dropout = getattr(base_model.config, 'dropout', 0.1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 确定基础模型的类型\n",
    "        self.model_type = base_model.config.model_type\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # 获取基础模型的名称\n",
    "        if hasattr(self.base_model, 'base_model'):\n",
    "            # 如果是PEFT模型，获取实际的基础模型\n",
    "            actual_base_model = self.base_model.base_model\n",
    "        else:\n",
    "            actual_base_model = self.base_model\n",
    "            \n",
    "        # 根据模型类型使用不同的方法获取表示\n",
    "        if self.model_type == 'bert':\n",
    "            # 直接调用bert层\n",
    "            if hasattr(actual_base_model, 'bert'):\n",
    "                outputs = actual_base_model.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                pooled_output = outputs.pooler_output\n",
    "            else:\n",
    "                # 如果是PEFT包装的模型\n",
    "                outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "                # 从隐藏状态中获取\n",
    "                if hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:\n",
    "                    pooled_output = outputs.hidden_states[-1][:, 0]\n",
    "                elif hasattr(outputs, 'last_hidden_state'):\n",
    "                    pooled_output = outputs.last_hidden_state[:, 0]\n",
    "                else:\n",
    "                    # 使用logits前的表示\n",
    "                    pooled_output = self.base_model.get_input_embeddings()(input_ids).mean(dim=1)\n",
    "                    \n",
    "        elif self.model_type == 'roberta':\n",
    "            # 对于RoBERTa，需要特殊处理\n",
    "            if hasattr(actual_base_model, 'roberta'):\n",
    "                # 直接调用roberta层\n",
    "                outputs = actual_base_model.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                pooled_output = outputs.last_hidden_state[:, 0]\n",
    "            else:\n",
    "                # 如果是PEFT包装的模型，使用不同的方法\n",
    "                # 临时设置输出隐藏状态\n",
    "                self.base_model.config.output_hidden_states = True\n",
    "                outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                \n",
    "                # 获取隐藏状态\n",
    "                if hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:\n",
    "                    # 使用最后一层的隐藏状态\n",
    "                    last_hidden_state = outputs.hidden_states[-1]\n",
    "                    pooled_output = last_hidden_state[:, 0]\n",
    "                else:\n",
    "                    # 如果没有hidden_states，尝试从基础模型获取\n",
    "                    # 这里我们需要手动提取特征\n",
    "                    base_model_output = actual_base_model.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    pooled_output = base_model_output.last_hidden_state[:, 0]\n",
    "                    \n",
    "        elif self.model_type == 'xlnet':\n",
    "            if hasattr(actual_base_model, 'transformer'):\n",
    "                outputs = actual_base_model.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                sequence_output = outputs.last_hidden_state\n",
    "            else:\n",
    "                self.base_model.config.output_hidden_states = True\n",
    "                outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                if hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:\n",
    "                    sequence_output = outputs.hidden_states[-1]\n",
    "                else:\n",
    "                    sequence_output = outputs.last_hidden_state\n",
    "                    \n",
    "            # 获取每个序列的最后一个有效token\n",
    "            if attention_mask is not None:\n",
    "                seq_lengths = attention_mask.sum(dim=1) - 1\n",
    "                batch_size = input_ids.size(0)\n",
    "                pooled_output = sequence_output[range(batch_size), seq_lengths]\n",
    "            else:\n",
    "                pooled_output = sequence_output[:, -1]\n",
    "                \n",
    "        else:\n",
    "            # 其他模型的通用处理\n",
    "            self.base_model.config.output_hidden_states = True\n",
    "            outputs = self.base_model(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "                pooled_output = outputs.pooler_output\n",
    "            elif hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:\n",
    "                pooled_output = outputs.hidden_states[-1][:, 0]\n",
    "            elif hasattr(outputs, 'last_hidden_state'):\n",
    "                pooled_output = outputs.last_hidden_state[:, 0]\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot extract pooled output from model type: {self.model_type}\")\n",
    "        \n",
    "        # 应用dropout\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # 主任务输出\n",
    "        main_logits = self.main_classifier(pooled_output)\n",
    "        \n",
    "        # 辅助任务输出\n",
    "        tense_logits = self.tense_classifier(pooled_output)\n",
    "        \n",
    "        return {\n",
    "            'main_logits': main_logits,\n",
    "            'tense_logits': tense_logits\n",
    "        }\n",
    "    \n",
    "# 创建一个函数来查找模型中的所有模块名称\n",
    "def find_all_linear_names(model):\n",
    "    \"\"\"\n",
    "    找出模型中所有的线性层名称\n",
    "    \"\"\"\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[-1])\n",
    "    \n",
    "    # 移除一些不应该被替换的层\n",
    "    if 'lm_head' in lora_module_names:\n",
    "        lora_module_names.remove('lm_head')\n",
    "    if 'classifier' in lora_module_names:\n",
    "        lora_module_names.remove('classifier')\n",
    "    if 'qa_outputs' in lora_module_names:\n",
    "        lora_module_names.remove('qa_outputs')\n",
    "    \n",
    "    return list(lora_module_names)\n",
    "\n",
    "# 修改主训练循环中的LoRA配置部分\n",
    "for model_class, tokenizer_class, pretrained_weights in MODELS:\n",
    "    print(f\"\\n\\n=== Training Model with LoRA: {pretrained_weights} ===\\n\")\n",
    "    \n",
    "    # Loading the data\n",
    "    train_corpus, test_corpus = master_corpus, test_corpus\n",
    "    \n",
    "    # 从本地加载预训练模型\n",
    "    local_path = LOCAL_MODEL_PATHS[pretrained_weights]\n",
    "    tokenizer = tokenizer_class.from_pretrained(local_path)\n",
    "    base_model = model_class.from_pretrained(\n",
    "        local_path,\n",
    "        num_labels=2,\n",
    "        output_hidden_states=True,\n",
    "        output_attentions=False\n",
    "    )\n",
    "    \n",
    "    # 在应用LoRA之前，确保模型配置正确\n",
    "    base_model.config.output_hidden_states = True\n",
    "    \n",
    "    # 根据模型类型配置LoRA\n",
    "    if base_model.config.model_type == 'xlnet':\n",
    "        # 对于XLNet，我们需要找到正确的模块名称\n",
    "        print(\"Finding XLNet modules...\")\n",
    "        linear_modules = find_all_linear_names(base_model)\n",
    "        print(f\"Found linear modules: {linear_modules}\")\n",
    "        \n",
    "        # XLNet特定的配置\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            target_modules=linear_modules,  # 使用找到的所有线性层\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "        )\n",
    "    elif base_model.config.model_type == 'bert':\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "        )\n",
    "    elif base_model.config.model_type == 'roberta':\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "        )\n",
    "    else:\n",
    "        # 通用配置 - 自动查找所有线性层\n",
    "        linear_modules = find_all_linear_names(base_model)\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            target_modules=linear_modules,\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "        )\n",
    "    \n",
    "    # 应用LoRA到基础模型\n",
    "    base_model = get_peft_model(base_model, lora_config)\n",
    "    \n",
    "    # 打印可训练参数信息\n",
    "    base_model.print_trainable_parameters()\n",
    "    \n",
    "    # 创建多任务数据集\n",
    "    train_dataset = MultiTaskClassificationDataset(train_corpus, train_corpus, tokenizer, max_len=128, is_test=False)\n",
    "    test_dataset = MultiTaskClassificationDataset(test_corpus, train_corpus, tokenizer, max_len=128, is_test=True)\n",
    "    \n",
    "    # 创建多任务模型（使用LoRA增强的基础模型）\n",
    "    model = MultiTaskLoRAModel(base_model, num_main_labels=2, num_tense_labels=train_dataset.num_tenses)\n",
    "    model.to(device)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    def collate_fn(batch):\n",
    "        input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "        attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "        main_labels = torch.stack([item['main_label'] for item in batch])\n",
    "        tense_labels = torch.stack([item['tense_label'] for item in batch])\n",
    "        return input_ids, attention_mask, main_labels, tense_labels\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    # 定义损失函数\n",
    "    main_criterion = nn.CrossEntropyLoss(weight=train_dataset.main_task_weights)\n",
    "    tense_criterion = nn.CrossEntropyLoss(weight=train_dataset.tense_weights)\n",
    "    \n",
    "    # 定义多任务损失权重\n",
    "    main_task_weight = 0.7\n",
    "    tense_task_weight = 0.3\n",
    "    \n",
    "    epochs = 10\n",
    "    \n",
    "    # 优化器 - 使用更高的学习率因为LoRA参数较少\n",
    "    optimizer = AdamW(model.parameters(), lr=3e-4, eps=1e-8)\n",
    "    \n",
    "    # 学习率调度器\n",
    "    total_train_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=int(0.1 * total_train_steps),  # 10% warmup\n",
    "        num_training_steps=total_train_steps\n",
    "    )\n",
    "    \n",
    "    # 训练循环\n",
    "    best_f1 = 0\n",
    "    for epoch in range(epochs):   \n",
    "        model.train()\n",
    "        \n",
    "        train_main_preds = []\n",
    "        train_main_labels = []\n",
    "        train_tense_preds = []\n",
    "        train_tense_labels = []\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i, (input_ids, attention_mask, main_labels, tense_labels) in enumerate(train_loader):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            main_labels = main_labels.to(device)\n",
    "            tense_labels = tense_labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            \n",
    "            # 计算多任务损失\n",
    "            main_loss = main_criterion(outputs['main_logits'], main_labels)\n",
    "            tense_loss = tense_criterion(outputs['tense_logits'], tense_labels)\n",
    "            \n",
    "            # 组合损失\n",
    "            loss = main_task_weight * main_loss + tense_task_weight * tense_loss\n",
    "            \n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # 记录预测结果\n",
    "            main_preds = torch.argmax(outputs['main_logits'], dim=1).cpu().numpy()\n",
    "            tense_preds = torch.argmax(outputs['tense_logits'], dim=1).cpu().numpy()\n",
    "            \n",
    "            train_main_preds.extend(main_preds)\n",
    "            train_main_labels.extend(main_labels.cpu().numpy())\n",
    "            train_tense_preds.extend(tense_preds)\n",
    "            train_tense_labels.extend(tense_labels.cpu().numpy())\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # 计算主任务指标\n",
    "        main_acc = accuracy_score(train_main_labels, train_main_preds)\n",
    "        main_prec = precision_score(train_main_labels, train_main_preds)\n",
    "        main_rec = recall_score(train_main_labels, train_main_preds)\n",
    "        main_f1 = f1_score(train_main_labels, train_main_preds)\n",
    "        \n",
    "        # 计算辅助任务指标\n",
    "        tense_acc = accuracy_score(train_tense_labels, train_tense_preds)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} - Loss: {total_loss/len(train_loader):.4f}\")\n",
    "        print(f\"Main Task - Accuracy: {main_acc:.4f}, Precision: {main_prec:.4f}, Recall: {main_rec:.4f}, F1: {main_f1:.4f}\")\n",
    "        print(f\"Tense Task - Accuracy: {tense_acc:.4f}\")\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_main_preds = []\n",
    "        val_main_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for input_ids, attention_mask, main_labels, _ in test_loader:\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                main_labels = main_labels.to(device)\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                main_preds = torch.argmax(outputs['main_logits'], dim=1).cpu().numpy()\n",
    "                \n",
    "                val_main_preds.extend(main_preds)\n",
    "                val_main_labels.extend(main_labels.cpu().numpy())\n",
    "        \n",
    "        val_f1 = f1_score(val_main_labels, val_main_preds)\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_model_state = model.state_dict()\n",
    "            print(f\"New best F1 score: {best_f1:.4f}\")\n",
    "    \n",
    "    # 加载最佳模型进行最终测试\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # 测试阶段 - 只关注主任务\n",
    "    model.eval()\n",
    "    test_main_preds = []\n",
    "    test_main_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, main_labels, _ in test_loader:  # 忽略时态标签\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            main_labels = main_labels.to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            \n",
    "            # 只获取主任务的预测\n",
    "            main_preds = torch.argmax(outputs['main_logits'], dim=1).cpu().numpy()\n",
    "            \n",
    "            test_main_preds.extend(main_preds)\n",
    "            test_main_labels.extend(main_labels.cpu().numpy())\n",
    "    \n",
    "    # 计算测试集指标 - 只计算主任务\n",
    "    main_acc = accuracy_score(test_main_labels, test_main_preds)\n",
    "    main_prec = precision_score(test_main_labels, test_main_preds)\n",
    "    main_rec = recall_score(test_main_labels, test_main_preds)\n",
    "    main_f1 = f1_score(test_main_labels, test_main_preds)\n",
    "    \n",
    "    print(f\"\\nFinal Test Results with LoRA:\")\n",
    "    print(f\"Main Task - Accuracy: {main_acc:.4f}, Precision: {main_prec:.4f}, Recall: {main_rec:.4f}, F1: {main_f1:.4f}\")\n",
    "    \n",
    "    # 保存模型和预测结果\n",
    "    model_name = pretrained_weights.replace(\"-\", \"_\")\n",
    "    \n",
    "    # 保存完整模型（包括LoRA权重）\n",
    "    torch.save(model.state_dict(), f'./saved_models/{model_name}_multitask_lora.pt')\n",
    "    \n",
    "    # 也可以只保存LoRA权重（更小的文件）\n",
    "    model.base_model.save_pretrained(f'./saved_models/{model_name}_lora_weights')\n",
    "    \n",
    "    # 保存预测结果\n",
    "    np.save(f'./saved_preds/{model_name}_main_preds_lora.npy', test_main_preds)\n",
    "    \n",
    "    print(f\"Saved LoRA model and predictions for {pretrained_weights}\")\n",
    "    print(f\"Best validation F1 score: {best_f1:.4f}\")\n",
    "\n",
    "# 如果需要加载保存的LoRA模型，可以使用以下代码：\n",
    "def load_lora_model(model_class, tokenizer_class, pretrained_weights, num_tense_labels):\n",
    "    \"\"\"\n",
    "    加载保存的LoRA模型\n",
    "    \"\"\"\n",
    "    # 加载基础模型\n",
    "    local_path = LOCAL_MODEL_PATHS[pretrained_weights]\n",
    "    base_model = model_class.from_pretrained(\n",
    "        local_path,\n",
    "        num_labels=2,\n",
    "        output_hidden_states=True,\n",
    "        output_attentions=False\n",
    "    )\n",
    "    \n",
    "    # 重新配置LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "    )\n",
    "    \n",
    "    # 根据模型类型调整target_modules\n",
    "    if base_model.config.model_type == 'bert':\n",
    "        lora_config.target_modules = [\"query\", \"key\", \"value\", \"dense\"]\n",
    "    elif base_model.config.model_type == 'roberta':\n",
    "        lora_config.target_modules = [\"query\", \"key\", \"value\", \"dense\"]\n",
    "    elif base_model.config.model_type == 'xlnet':\n",
    "        lora_config.target_modules = [\"q\", \"k\", \"v\", \"o\", \"ff.0\", \"ff.2\"]\n",
    "    \n",
    "    # 应用LoRA\n",
    "    base_model = get_peft_model(base_model, lora_config)\n",
    "    \n",
    "    # 创建多任务模型\n",
    "    model = MultiTaskLoRAModel(base_model, num_main_labels=2, num_tense_labels=num_tense_labels)\n",
    "    \n",
    "    # 加载保存的权重\n",
    "    model_name = pretrained_weights.replace(\"-\", \"_\")\n",
    "    model.load_state_dict(torch.load(f'./saved_models/{model_name}_multitask_lora.pt'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 额外的LoRA配置选项示例\n",
    "def get_advanced_lora_config(model_type):\n",
    "    \"\"\"\n",
    "    获取更高级的LoRA配置\n",
    "    \"\"\"\n",
    "    if model_type == 'bert':\n",
    "        return LoraConfig(\n",
    "            r=16,  # 增加秩以提高性能\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\"query\", \"key\", \"value\", \"dense\", \"output.dense\"],  # 包含更多层\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"lora_only\",  # 只在LoRA层使用偏置\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            modules_to_save=[\"classifier\"],  # 保存分类器层\n",
    "        )\n",
    "    elif model_type == 'roberta':\n",
    "        return LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\"query\", \"key\", \"value\", \"dense\", \"output.dense\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"lora_only\",\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            modules_to_save=[\"classifier\"],\n",
    "        )\n",
    "    elif model_type == 'xlnet':\n",
    "        return LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\"q\", \"k\", \"v\", \"o\", \"ff.0\", \"ff.2\", \"layer_norm\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"lora_only\",\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            modules_to_save=[\"classifier\"],\n",
    "        )\n",
    "    else:\n",
    "        # 默认配置\n",
    "        return LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "        )\n",
    "\n",
    "# 用于评估LoRA效率的函数\n",
    "def print_lora_statistics(model):\n",
    "    \"\"\"\n",
    "    打印LoRA模型的统计信息\n",
    "    \"\"\"\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        total_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    \n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Trainable percentage: {100 * trainable_params / total_params:.2f}%\")\n",
    "    \n",
    "    # 计算内存使用\n",
    "    param_memory = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    buffer_memory = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "    total_memory = param_memory + buffer_memory\n",
    "    \n",
    "    print(f\"Model memory usage: {total_memory / 1024**2:.2f} MB\")\n",
    "    print(f\"Trainable parameters memory: {trainable_params * 4 / 1024**2:.2f} MB\")  # 假设float32\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for BERT Large: 0.8838\n",
      "Recall for BERT Large: 0.8550\n",
      "F1 Score for BERT Large: 0.8691\n",
      "\n",
      "Precision for RoBERTa Large: 0.9194\n",
      "Recall for RoBERTa Large: 0.8659\n",
      "F1 Score for RoBERTa Large: 0.8918\n",
      "\n",
      "Precision for XLNet Large: 0.9044\n",
      "Recall for XLNet Large: 0.8591\n",
      "F1 Score for XLNet Large: 0.8812\n",
      "\n",
      "Precision for Ensemble Model: 0.9143\n",
      "Recall for Ensemble Model: 0.8672\n",
      "F1_ensemble Score on Test Set: 0.8901\n"
     ]
    }
   ],
   "source": [
    "# 加载所有模型的预测结果\n",
    "bert_large = np.load(\"./saved_preds/bert_large_uncased_preds.npy\")\n",
    "roberta_large = np.load(\"./saved_preds/roberta_large_preds.npy\")\n",
    "xlnet_large = np.load(\"./saved_preds/xlnet_large_cased_preds.npy\")\n",
    "\n",
    "# 投票集成\n",
    "final_pred = bert_large + roberta_large + xlnet_large\n",
    "preds = (final_pred >= 2).astype(int)  # 至少两个模型认为是反事实才预测为 1\n",
    "\n",
    "# 打印F1\n",
    "# 加载测试集的标签\n",
    "test_labels = pd.read_csv(\"data_aug/test.csv\")[\"gold_label\"].values\n",
    "\n",
    "bert_precision = precision_score(test_labels, bert_large)\n",
    "bert_recall = recall_score(test_labels, bert_large)\n",
    "bert_f1 = f1_score(test_labels, bert_large)\n",
    "\n",
    "roberta_precision = precision_score(test_labels, roberta_large)\n",
    "roberta_recall = recall_score(test_labels, roberta_large)\n",
    "roberta_f1 = f1_score(test_labels, roberta_large)\n",
    "\n",
    "xlnet_precision = precision_score(test_labels, xlnet_large)\n",
    "xlnet_recall = recall_score(test_labels, xlnet_large)\n",
    "xlnet_f1 = f1_score(test_labels, xlnet_large)\n",
    "\n",
    "# 计算最终模型 F1 分数\n",
    "f1_ensemble = f1_score(test_labels, preds)\n",
    "precision_ensemble = precision_score(test_labels, preds)\n",
    "recall_ensemble = recall_score(test_labels, preds)\n",
    "\n",
    "# 打印每个模型的 F1 分数\n",
    "\n",
    "print(f\"Precision for BERT Large: {bert_precision:.4f}\")\n",
    "print(f\"Recall for BERT Large: {bert_recall:.4f}\")\n",
    "print(f\"F1 Score for BERT Large: {bert_f1:.4f}\\n\")\n",
    "\n",
    "print(f\"Precision for RoBERTa Large: {roberta_precision:.4f}\")\n",
    "print(f\"Recall for RoBERTa Large: {roberta_recall:.4f}\")\n",
    "print(f\"F1 Score for RoBERTa Large: {roberta_f1:.4f}\\n\")\n",
    "\n",
    "print(f\"Precision for XLNet Large: {xlnet_precision:.4f}\")\n",
    "print(f\"Recall for XLNet Large: {xlnet_recall:.4f}\")\n",
    "print(f\"F1 Score for XLNet Large: {xlnet_f1:.4f}\\n\")\n",
    "\n",
    "print(f\"Precision for Ensemble Model: {precision_ensemble:.4f}\")\n",
    "print(f\"Recall for Ensemble Model: {recall_ensemble:.4f}\")\n",
    "print(f\"F1_ensemble Score on Test Set: {f1_ensemble:.4f}\")\n",
    "\n",
    "# 保存最终集成结果\n",
    "# np.save(\"final_ensemble_preds.npy\", preds)\n",
    "# print(\"✅ 集成预测完成，结果已保存至 final_ensemble_preds.npy\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "voting of 3 transformer_baseline.ipynb",
   "provenance": [
    {
     "file_id": "1DTbYI_bgQljngtN9b09S2znGaqUsDvu1",
     "timestamp": 1582825796897
    },
    {
     "file_id": "1WlpZOxP3YgwllqKn2H3rcBJuhGe6H2u3",
     "timestamp": 1582823073870
    },
    {
     "file_id": "1vWlKxVBu82x73C4ZI0aqsl8gXi0OTpsW",
     "timestamp": 1582730282048
    }
   ]
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09c632c1b0884b9797c4b3887a6fd456": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b9747e0a3494736a2bc512470bf9aa6",
      "placeholder": "​",
      "style": "IPY_MODEL_222057aca2134f2a8fb97cfba1762b08",
      "value": "100% 362/362 [00:00&lt;00:00, 13.5kB/s]"
     }
    },
    "12bf5fb0b8c64eabbaf65fe81075a378": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d23a6d4541f145eeb6807c4d4b4691af",
      "placeholder": "​",
      "style": "IPY_MODEL_845a78a9c4af4c12882d24086b8184a9",
      "value": "100% 232k/232k [00:00&lt;00:00, 427kB/s]"
     }
    },
    "222057aca2134f2a8fb97cfba1762b08": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2334f62b75f1418fad7e4ba376bc0e72": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "271dbf24b1c844c8a791f5ad8a998a6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5aa489639bb24075a68a4f8520e34601",
      "placeholder": "​",
      "style": "IPY_MODEL_afba953fb4f84423be40d5d895b63cfe",
      "value": "100% 1.34G/1.34G [01:55&lt;00:00, 11.6MB/s]"
     }
    },
    "57329d636a8c4699a70a361068e0391f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "5aa489639bb24075a68a4f8520e34601": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b9747e0a3494736a2bc512470bf9aa6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ba56cd2cdfb4bf4882345509cef87c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d90c483577cf46f1ab346803de5300ed",
       "IPY_MODEL_09c632c1b0884b9797c4b3887a6fd456"
      ],
      "layout": "IPY_MODEL_a73534fb52994a349cc63484f33f819c"
     }
    },
    "8263a27d38a349928eb0c45dcf3e55e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "845a78a9c4af4c12882d24086b8184a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8bc9af57a8cd4f97b179aeb765aaf99b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "8f9cde4c46234c38b16bea53f17cb6fa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d424363671349578a04fd8246639f94": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd2dae251daf4e1a9d9ab4c4bbc41d25",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_57329d636a8c4699a70a361068e0391f",
      "value": 231508
     }
    },
    "a73534fb52994a349cc63484f33f819c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "afba953fb4f84423be40d5d895b63cfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b5ecca696f1e4bf89284de2a43926c1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f22370b8f1134359b123f9f06ab330b7",
       "IPY_MODEL_271dbf24b1c844c8a791f5ad8a998a6c"
      ],
      "layout": "IPY_MODEL_8f9cde4c46234c38b16bea53f17cb6fa"
     }
    },
    "bd2dae251daf4e1a9d9ab4c4bbc41d25": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d23a6d4541f145eeb6807c4d4b4691af": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d826582003704642a244f9e5fb5e41ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9d424363671349578a04fd8246639f94",
       "IPY_MODEL_12bf5fb0b8c64eabbaf65fe81075a378"
      ],
      "layout": "IPY_MODEL_ecc691742ac946ebbe542a517ea38cbe"
     }
    },
    "d90c483577cf46f1ab346803de5300ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2334f62b75f1418fad7e4ba376bc0e72",
      "max": 362,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8bc9af57a8cd4f97b179aeb765aaf99b",
      "value": 362
     }
    },
    "ecc691742ac946ebbe542a517ea38cbe": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f22370b8f1134359b123f9f06ab330b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f24871c5b7f74ec0a930b764fe954229",
      "max": 1344997306,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8263a27d38a349928eb0c45dcf3e55e6",
      "value": 1344997306
     }
    },
    "f24871c5b7f74ec0a930b764fe954229": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
